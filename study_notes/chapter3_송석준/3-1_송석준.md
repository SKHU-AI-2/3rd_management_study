# Simple Linear Regression
https://velog.io/@suwdle/ISL-Linear-Regression
### Linear Regression
지도학습에서 가장 쉬운 접근법 중 하나인 선형 회귀를 다룬다. 선형회귀는 연속적인 답을 예측하는데 유용한 방법론이다. 오랜 기간 사용되었기 때문에 다른 통계적 학습 접근법과 비교해 단순해 보일 수 있지만 아직도 많이 쓰이는 접근론이다.

게다가 더 새로운 접근법에 대한 점핑 포인트가 되어준다. 많은 통계적 학습 방법론들은 선형 회귀의 일반화나 확장이라고 볼 수 있다. 이런 방법론들을 배우기 전에 선형 회귀를 이해하는 것이 중요하다. 

**선형 회귀로 알 수 있는 것**
- feature와 y 값 사이의 어떠한 관계가 있는가?
- feature와 y 값 사이의 관계가 얼마나 강한가?
- feature들 중 어떤 feature가 y와 연관되어 있나?
- 관계가 선형적인가?
- feature 사이의 synergy(interaction)이 있는가?

### 3.1 Simple Linear Regression

가장 기초적인 선형 회귀는 이름 그대로 연속적인 변수 Y를 예측하기 위해 단일 변수 X를 사용하는 직관적인 접근법이다. X와 Y가 대략 선형관계라는 것을 가정한다. 수식으로 표현하면

![](https://velog.velcdn.com/images/suwdle/post/ed7b16d2-4aac-4372-a192-9a8e51e86cd2/image.png)

이따금 Y를 X 상에 회귀한다고 표현하기도 한다.(Y on X, Y onto X)
B0는 y 절편을 나타내고, B1은 기울기를 나타내는 알수 없는 상수들이다. 이들은 모델의 coefficients 또는 parameter라고 불린다. 훈련 데이터를 이 coefficients를 추정하기 위해 사용해 X에 대한 Y의 예측값을 다음과 같이 계산할 수 있다:
![](https://velog.velcdn.com/images/suwdle/post/830b679f-4225-4540-80a2-cc1a20387583/image.png)
### 3.1.1 Estimating the Coefficients

학습 과정에서 coefficients는 알 수 없다. 그래서 반드시 이들을 추정하기 위해 데이터를 사용해야 한다. 각각 X와 Y 쌍인 n개의 데이터 포인트가 있을 때, 우리 목표는 이 데이터셋에 잘 맞는 B0와 B1 값을 구하는 것이다. 다른 말로 우리는 n개의 데이터 포인트에 가능한 만큼 가까운 하나의 선을 만드는 intercept와 slope를 구해야 한다.
closeness를 구하는 방법은 많지만, 가장 일반적인 접근법은 least squares criteron을 최소화하는 것이다. 
실제 Y 값과 예측한 값 사이의 차이를 통해 계산한 후 제곱해야 하는데, 수식을 통해 나타내면 다음과 같다:
![](https://velog.velcdn.com/images/suwdle/post/224fc6c4-3e62-4426-8447-9e05fb9dcf6c/image.png)
least squares 접근법은 RSS를 최소화하는 B0, B1를 찾는다. 이는 학습 과정이 따로 필요하지 않고 바로 수식으로 계산할 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/a3beb4a3-a6e0-4e14-a5d3-11ac5ce5466d/image.png)
least squares coeffient 추정법을 정의하는 수식이다.
이 수식을 계산해 나온 coefficients는 RSS를 최소화한다.

### 3.1.2 Assessing the Accuracy of the Coefficient Estimates

2장에서 나온 내용에서 랜덤한 오차의 평균인 입실론 값이 있었다. f가 만약 선형 함수라면 우리는 이 관계를 다시 쓸 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/1030f022-9374-4a85-8016-d67c6b79783a/image.png)

B0는 X가 0일 때 Y의 예측값이고, B1는 Y의 평균적인 증가율을 나타내는 기울기이다. 입실론 오차는 이 단순한 모델에서 우리가 놓친 것들을 나타낸다. 진짜 관계는 선형이 아닐 가능성이 높고 다른 변수들이 Y에 영향을 끼칠 수 있고 측정 오차가 있을 수 있다. 우리는 이런 오차가 X로부터 독립적이라고 가정한다.

이 식에서의 모델은 X와 Y 사이의 진짜 관계에 가장 가까운 선형 관계인 populatin regression line을 정의한다. least squares regression coefficient 추정은 least squares line을 정의한다. 진짜 관계는 현실 데이터에서는 일반적으로 알 수 없지만, least squares line은 항상 계산할 수 있다.
하지만 populatin regression line은 관찰할 수 없다.
만약 우리가 같은 데이터셋 안에서 일부를 추출해 least squares line을 만들고, 또 다시 일부를 추출해 leas squares line을 만든다면 이 둘은 서로 다를 수 있다. 하지만 population regression line은 항상 같을 것이다.
![](https://velog.velcdn.com/images/suwdle/post/21cce306-cabd-46cd-a94b-77d550984da7/image.png)

이 그래프의 빨간 선은 population regression line을 나타내고, 파란 선은 least squares line을 나타낸다.
오른쪽의 하늘색 선은 일부 데이터포인트만 사용한 것이다.

하나의 데이터셋에서 서로 다른 직선이 변수 사이의 관계를 묘사하니 혼란스러울 수 있지만 근본적으로 이 두 직선의 개념은 거대한 모집단의 특성들을 추정하기 위해 표본으로부터 정보를 사용하는 통계적인 접근 방식의 확장판이다.

즉, 인류라는 모집단(population)의 평균 키를 측정하려고 하는데, 모든 사람들의 키를 측정하는 것은 불가능할 것이다. 대신 표본집단의 키를 사용해 합리적으로 실제 인류의 평균 키를 추정할 수 있다.
이 둘은 실제로는 차이가 있겠지만, 실제 평균에 대한 좋은 추정을 제공해준다. 

이 예시와 똑같이, 우리가 알지 못하는 coefficients는 모집단의 regression line이지만 우리는 대부분의 경우 모집단 전체가 아니라 일부인 표본 집단에 대해 계산하기 때문에 population regression line과 least squares line이 차이가 날 수 있는 것이다.

선형 회귀와 확률 변수의 평균 추정 간의 유사성은 편향 개념을 바탕으로 한 적절한 비유이다(The analogy between linear regression and estimation of the mean of a
 random variable is an apt one based on the concept of bias.).
 우리가 표본 평균을 모집단 평균을 추정하기 위해 사용한다면, 이 추정은 다음과 같은 의미에서 비편향적이다: 평균적으로 표본 평균과 모집단 평균이 같다고 기대한다. 말이 어렵다...
이것이 정확하게 의미하는 바는 어떠한 특정한 데이터 포인트들에 대해서는 모집단 평균을 과대평가하고, 다른 경우에는 과소평가할 수 있다는 것이다.

하지만 우리가 추정하기 위해 엄청나게 많은 데이터 포인트를 사용한다면 이 평균은 모집단 평균과 같을 것이다. 따라서 비편향적인 추정기는 과대, 과소평가를 하지 않아야 한다. 추정이 비편향적이고 최대한 실제 값과 가깝게 하기 위해서는 많은 수의 데이터가 필요하다. 그럼 이런 추정들의 평균적인 값이 나올 것이다. 즉 적은 수의 데이터를 사용하면 과대, 과소 평가를 하게 되지만 더 많은 수의 데이터를 사용하면 이런 경향을 평균적으로 줄일 수 있다는 것이다. 이런 사실을 위 그래프의 오른쪽에서 볼 수 있다.

우리가 많은 데이터셋을 사용해 표본 평균과 모집단 평균을 같은 상태로 만들어도 각각 하나의 추정에 대해서는 오차가 발생할 것이다. 이 단일 추정치가 얼마나 떨어져있는지 알기 위해서 일반적으로 표준 오차(standard error)를 사용한다.
![](https://velog.velcdn.com/images/suwdle/post/ea6bb18c-bf2a-498f-8a39-97ec6d36a23f/image.png)

여기서 시그마는 Y의 표준편차이다. 
표준 오차는 예측 값의 실제 값으로부터의 오차의 평균에 대해 알려준다. 위 방정식은 Y의 표준편차가 n이 커질수록 영향력이 적어진다는 걸 알 수 있게 해준다. 즉. n이 커질수록 표준오차는 더 작아질 것이다. 

이 방식으로, B0와 B1에 대해서도 표준 오차 공식을 작성할 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/82f80319-ec1e-4f6f-acb5-0510ce962299/image.png)
여기서 시그마^2은 입실론 값의 분산이다. 우리는 이 식을 strict하게 만들기 위해 각 포인트에 대한 입실론이 공통된 분산을 지니고 서로 상관하지 않다는 것을 가정해야 한다. 실제 데이터의 경우엔 그럴 수 없지만, 그럼에도 이 식은 좋은 접근법으로서 유효하다.

SE(B1)이 xi가 더 퍼질 때(즉 분산이 커질 때) 더 작아진다는 점에 집중하자. 직관적으로 기울기를 추정할 때 더 많은 영향력을 가지고 있다는 것을 알 수 있다.

또한 SE(B0)에서 x의 평균이 0이 될 때 SE(^u)와 식이 같아짐을 알 수 있다. 이경우에는 B^0는 y의 평균이 될 것이다.
일반적으로 시그마 값은 알 수 없지만 데이터로부터 추정할 수 있다. 이 시그마의 추정치는 residual standard error라고 부르며
![](https://velog.velcdn.com/images/suwdle/post/553607ee-0693-446f-a63f-e4031cebebd4/image.png)
이와 같다. 
엄밀히 보면 시그마 값도 데이터로부터 추정한 것이기 때문에 SE(B1)도 정확한 값이 아닌 추정한 값이긴 하다.
표준 오차들은 신뢰구간을 계산하는 데 사용할 수 있다. 95% 신뢰구간은 95% 확률로 값들이 존재하는 영역으로 정의할 수 있다. 이 구간은 우리가 알 수 없는 실제 파라미터 값을 포함할 수 있으며 데이터 샘플로부터 계산된 상한과 하한으로 정의된다. 95% 신뢰구간은 만약 우리가 반복된 샘플들을 사용하고 각 샘플에 대해 신뢰구간을 구축한다면 구간의 95%는 파라미터의 알려지지 않은 실제 값을 포함할 수 있다는 것이다. 

선형회귀에서는 B1에 대한 95% 신뢰구간이 다음과 같이 정의된다.
![](https://velog.velcdn.com/images/suwdle/post/c3384f45-3452-4b10-b338-27c6ff196734/image.png)
![](https://velog.velcdn.com/images/suwdle/post/a0869c4b-6b1c-401f-986b-8ba220b83a19/image.png)

B0도 이와 동일하다.

표준 오차는 coefficents에 대한 가설 검정을 할 때 사용할 수 있다. 일반적인 가설 검정은 X와 Y 사이의 아무런 관계가 없다는 것을 가정하는 null hypothesis와 그 반대인 alternative hypothesis로 나눈다.
null hypothesis를 수식으로 표현하면 B1 = 0이다.
그럼 Y = B0 + 입실론이 되는데, 이는 X와 Y가 아무 연관이 없다는 것이다.
귀무가설을 검증하기 위해서는 B1에 대한 추정이 0으로부터 확실하게 멀리 떨어져 있어야 한다.
이는 B1의 정확성에 달려있으며, 그러므로 SE(B1)에 좌우된다. 이 값이 작다면, B1의 추정치가 작더라도 B1 =/=0이라는 강한 증거가 되어준다. 그 반대로 크다면 귀무가설을 기각하기 위해서는 B1의 추정치가 커야할 것이다. 이걸 측정하기 위한 걸 t 통계량이라고 하는데, 수식으로 표현하면 다음과 같다.
![](https://velog.velcdn.com/images/suwdle/post/dd5864f0-bec6-46b0-941e-2069c34bfd5a/image.png)

t-분포는 종 모양을 가지며, n이 대략 30보다 클 경우 표준 정규 분포와 매우 유사하다. 따라서 우리는 B1 = 0이라고 가정했을 때 절댓값이 |t|와 같거나 더 큰 값을 관찰할 확률인 p-value를 쉽게 계산할 수 있다. p값이 작다는 것은, 귀무가설이 참일 때 t값을 가질 확률이 무척이나 낮다는 것이다. 작은 p-값을 통해 X와 Y 간에 연관성이 있다고 추론할 수 있다. 그러므로 p-value가 매우 작을 경우 우리는 귀무가설을 기각할 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/9e0d59ca-397c-4dad-9bee-f0391e4b8854/image.png)


이 부분이 이해가 안되어서 GPT의 설명을 덧붙인다.

> t-통계량(t-statistic)과 p-값(p-value)의 관계는 통계적 가설 검정에서 중요한 역할을 합니다. 이 두 개념을 자세히 설명하겠습니다.
### 1. t-통계량 (t-statistic)
t-통계량은 두 집단의 평균 차이 또는 회귀 분석에서의 기울기와 같은 추정치가 표본의 변동성에 비해 얼마나 유의미한지를 평가하는 지표입니다. 
t-통계량은 해당 파라미터가 0(즉, 관계가 없다는 가설)에 비해 얼마나 떨어져 있는지를 나타냅니다.
### 2. p-값 (p-value)
p-값은 귀무 가설이 참일 때, 관찰된 t-통계량과 같은 극단적인 값을 얻을 확률을 나타냅니다. 즉, p-값은 다음과 같이 해석할 수 있습니다:
- p-값이 작을수록 (일반적으로 0.05 또는 0.01 이하) 귀무 가설을 기각할 가능성이 높습니다.
- p-값이 클수록 귀무 가설을 기각하기 어렵습니다.
### 3. t-통계량과 p-값의 관계
t-통계량과 p-값은 밀접한 관계가 있습니다. t-통계량이 클수록 (즉, 추정된 파라미터가 0에서 멀리 떨어질수록), p-값은 작아집니다. 이는 다음과 같은 이유에서입니다:
- t-통계량이 높으면, 귀무 가설(관계 없음)이 참일 때 그러한 값을 얻을 확률이 낮아집니다.
- 따라서 p-값은 이러한 극단적인 결과를 나타내는 확률로, t-통계량이 높을수록 p-값은 작아집니다.
### 4. 예시
- 예를 들어, t-통계량이 2.5로 계산되었다고 가정해 봅시다. 이 경우, t-분포에서 2.5 이상의 값을 가질 확률(즉, p-값)은 일반적으로 작습니다. 이로 인해 우리는 귀무 가설을 기각하고 두 변수 간의 관계가 존재한다고 결론지을 수 있습니다.
### 요약
t-통계량은 데이터에서 얻은 추정치가 0과 얼마나 떨어져 있는지를 측정하는 반면, p-값은 이 값이 우연에 의해 나타날 가능성을 나타냅니다. 이 두 값은 통계적 가설 검정의 핵심 요소로, 데이터 분석에서 중요한 결정을 내리는 데 도움을 줍니다.

### Assessing the Accuracy of the Model

이제 선형 회귀 모델이 잘 fit했는지 평가할 차례이다. 주로 두가지 방법으로 나뉘는데, RSE와 R^2 계수이다(주로 회귀계수로도 불린다).

**Residual Standard Error**
각 데이터 포인트에 대한 오차 입실론의 존재 때문에, 우리가 실제 회귀선을 알더라도 우리는 완벽히 X로부터 Y를 예측하는 것이 불가능하다. RSE는 입실론 표준편차의 추정치이다. 거칠게 말하자면, 실제 회귀선으로부터 정답의 편차에 대한 평균이다. (모델이 실제 회귀선이라면)
![](https://velog.velcdn.com/images/suwdle/post/2c914254-e423-4677-b08f-d346c40e0430/image.png)
이는 모델이 아무리 실제 회귀선에 가까울지라도 평균적으로 이정도의 오차를 지닌다는 것이다.
이를 통해 오차 비율을 계산할 수도 있다.
RSE는 모델의 데이터에 대한 lack of fit을 측정하는 지표로 여겨진다. 만약 모델로부터의 예측이 실제 값과 매우 가깝다면 RSE는 작아질 것이고, 모델이 데이터에 잘 학습되었다고 할 수 있다.
RSE가 가장 작을 경우에는 실제 f를 예측해 오차 = 입실론 값인 경우일 것이다.

**R^2 statics**
RSE는 모델의 lack of fit에 대한 절대적인 지표이다. 하지만 Y 값들을 이용해 측정하기 때문에 어떤 것이 좋은 값인지 확고히 알 수 없다(scale problem). R2 계수는 대안적인 지표를 제시한다.
확률의 형태를 띄고 있어 Y의 scale로부터 독립적이다.
![](https://velog.velcdn.com/images/suwdle/post/0db67add-209c-4db7-bf41-a67e67cec1cf/image.png)
TSS는 Total Sum of Squares로, 이와 같이 계산한다:
![](https://velog.velcdn.com/images/suwdle/post/adaedea5-f67f-4ff7-bd6b-bd4e73d994bb/image.png)
TSS는 Y의 전체 분선을 측정하고, 회귀가 작동하기 전의 Y 만으로 측정되는 가변성(variability)의 양이라 생각할 수 있다. 대조적으로 RSS는 회귀가 적용된 후 설명되지 않고 남은 가변성의 양이라 할 수 있다.
그러므로 TSS-RSS는 회귀 모델이 설명하는 y의 가변성을 측정한다고 볼 수 있다.
1에 가까울수록 회귀모델이 더 잘 설명하는 것이다.
R2 계수는 X와 Y사이의 선형 관계를 측정하는 수단이므로, 상관관계와도 연관이 있다.
![](https://velog.velcdn.com/images/suwdle/post/cc118f2c-a260-403a-bfa9-78cad2a96e9d/image.png)
회귀 계수 대신 상관계수를 사용할 수도 있을 것이다. 실제로, 선형 회귀에서 회귀계수는 상관계수의 제곱이다. 하지만 다중 회귀에서는 그대로 이 개념을 확장해서 사용할 수 없다. 상관관계는 단일 변수들 간의 관계를 수량화하는 것이기 때문이다.