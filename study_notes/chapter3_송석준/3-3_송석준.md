# 3.3.1 Qulitative Predictors
지금까지의 논의에서 선형 회귀 모델의 모든 변수는 연속적인 변수였다. 하지만 실전에서는 어떤 변수들은 categorical한 경우가 있다.
먼저 이진 변수의 경우부터 어떻게 처리해야 하는지 살펴보자.
이 경우에는 선형 회귀 모델과의 동시 처리가 매우 간단해진다.
간단히 0과 1로 이루어진 더미 변수를 만들면 된다. 그리고 회귀 방정식의 변수로서 사용하면 된다.

![](https://velog.velcdn.com/images/suwdle/post/e5627978-d518-4569-8612-41eb3d460f24/image.png)

![](https://velog.velcdn.com/images/suwdle/post/a4aafc07-c19e-4c6f-b803-bdc38a6204bc/image.png)

연속적인 값이 아니기 때문에 일반적인 회귀모델의 가중치 값과 다른 의미를 부여할 수 있는데, B0는 x가 0인 데이터의 y값의 평균이라고 할 수 있고, B0+B1은 x가 1인 데이터의 y값의 평균이라고 할 수 있다.
자연스럽게 B1은 x가 0인 데이터와 1인 데이터의 차이의 평균이 될 것이다.
하지만 이 방법은 문제가 있다. 0과 1의 label을 어디까지나 멋대로 붙인 것이기 때문에, 회귀 모델 적합에는 영향을 끼치지 못하지만 계수의 해석에는 영향을 미친다. 이런 arbitary한 방법을 사용하면 p-value가 높게 측정된다. p-value는 arbitary한 선택에 영향을 받기 때문이다.
추가적인 설명을 덧붙이자면, B0가 전체 평균이 아닌 x가 0인 데이터의 평균이기 때문에 계수를 해석할 때 악영향을 끼칠 수 있다. B1가 x가 0일 때는 영향이 없고, B0를 일종의 default 값으로 해석할 여지가 있다. 특히 블랙박스 문제를 해결할 때 이런 위험에 빠질 우려가 있는 것 같다.

대안적인 방법으로 0/1 코딩 대신 -1/1 코딩을 사용해 단점을 개선할 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/9fb0240f-15c7-41e1-b807-0a939f9ce4ff/image.png)
이 경우 B0는 전체 y값의 평균이 될 것이고, B1는 평균 잔차. 코딩하는 방법만 다를 뿐 예측값은 같게 나온다.
이 방식에서는 B0도 실제 default 값이고, x가 0일 때도 B1의 값이 예측에 영향을 주기 때문에 계수 해석의 측면에서 더 유리하다고 볼 수 있다.

###  Qualitative Predictors with More than Two Levels
label이 더 많은 변수에 대해서는 어떻게 대응할 수 있을까?
하나의 더미 변수로는 표현할 수 없고, 여러 개를 사용해야 한다.
예를 들어 region이라는 변수에 East, WEST, South라는 3개의 label이 있다고 하자.
이 경우에는 2개의 이진 더미 변수가 필요하다.
![](https://velog.velcdn.com/images/suwdle/post/11bb44e7-bc43-4305-8533-202c97484fa3/image.png)
![](https://velog.velcdn.com/images/suwdle/post/aade9087-efed-4b81-8866-3dfc25b017cf/image.png)

x1 == 1일 때 South, x2 == 1일 때 West, 둘 다 0일 때는 East이다.
이 경우 B0는 x가 East인 데이터의 y값 평균이고, B1은 East와 South의 y값 잔차 평균이다. B2는 East와 West의 잔차 평균일 것이다.
이런 코딩 방식에선 East와 같이 더미 변수가 없는 label이 생기게 되는데 이를 baseline이라고 부른다.
이 방식에서도 이진 변수에 대해 원핫 인코딩을 적용했을 때와 동일한 문제가 발생한다. baseline 선택이 arbitary하고, 최종적인 예측이 이 선택에 관계없이 결정된다. 하지만 p-value와 각계수는 baseline 선택에 영향을 받기 때문에 높게 측정되는 것이다.
이런 상황에서는 개별 계수에 의존하는 것이 아니라 F-test를 통해 귀무가설을 기각할 수 있는지 여부를 판단해야 한다.
categorical 변수를 처리하기 위한 많은 코딩 방식이 있지만 이 책의 범위에서는 다루지 않는다.

### 3.3.2 Extensions of the Linear Model
일반적인 선형 회귀 모델은 해석 가능한 결과를 제공하고 현실 세계의 문제에 잘 적용된다. 그러나, 실전에서는 위험할 수 있는 제한적인 가정을 전제로 한다. 선형 회귀 모델의 가장 중요한 두 전제가 변수들과 정답 사이의 관계가 선형적(linear)이고 가산적(additive)이라는 것이다.
가산성 가정은 각 변수와 y와의 관계가 다른 변수와 독립적이라는 것을 의미한다. 선형성 가정은 X의 값과 상관없이 X의 변화에 따라 Y의 변화가 일정하다는 것을 의미한다.

### Removing the Additive Assumption

다항 선형 회귀에서 한 변수가 prediction에 끼치는 영향은 다른 변수의 값과 상관없이 독립적으로 문제가 설정되어 있었다. 하지만 이 가정은 틀릴 수도 있다. X1의 증가가 X2의 계수(기울기)에 영향을 끼칠 수도 있기 때문이다. 예를 들면 라디오 광고량이 늘어날수록 TV 광고의 효율이 좋아진다면, 라디오 광고량의 증가와 TV광고의 계수는 비례 관계에 있을 것이다. 하지만 simple model은 이런 점을 반영하지 못한다.
![](https://velog.velcdn.com/images/suwdle/post/e4d437eb-7ad5-4074-baa2-c101753c9f19/image.png)
이 회귀 식에서는 X1이 아무리 증가해도 X2의 계수에는 영향을 끼치지 못하고, 그 반대의 경우도 마찬가지이다.
이런 단점을 극복하기 위한 한가지 방법이 X1과 X2의 곱의 계수인 interaction term을 추가하는 것이다. 
![](https://velog.velcdn.com/images/suwdle/post/84a8cc2c-7ed3-4280-8dc5-d375412273f8/image.png)
이 항의 추가가 어떻게 가산성 가정을 완화할 수 있을까?
![](https://velog.velcdn.com/images/suwdle/post/c446c15c-d01f-4f06-9d87-1d4c8e679efb/image.png)
간단하게 우항을 변경한 것만으로 상호작용 항의 추가가 어떻게 가산성 가정을 완화하는지 알 수 있다. 이제 X1과 Y의 관계는 상수가 아니라 X2의 값에 따라서 변화할 수 있다.

만약 상호작용 항의 p-value가 매우 작지만, 다른 독립적 항들은 그렇지 않다면 어떻게 모델을 작성해야 할까? 계층적 원칙에 따라, 상호작용 항을 포함하려면, 독립적인 항들도 포함해야 한다. X1과 X2의 상호작용이 중요해보인다면, 독립적인 항들의 p-value가 높아도 그들을 모델에 포함해야 한다는 것이다.

이 원칙의 근거는 상호작용 항이 중요하다면, X1 또는 X2의 계수가 정확히 0인지 여부는 거의 중요하지 않다는 것이다. 또한 상호작용 항은 일반적으로 X1 및 X2와 상관관계가 있으므로 이를 제외하면 상호작용의 의미가 달라질 수 있다.

위에서는 두 변수가 모두 연속적인 변수인 경우였지만, 만약 한 변수는 categorical 변수라면 상호작용 항을 어떻게 만들 수 있을까?
![](https://velog.velcdn.com/images/suwdle/post/e927e5e7-4605-4682-bded-a084ffa742dd/image.png)
우선 독립적인 회귀식부터 보면, 학생인지 아닌지에 따라 B0 + B2와 B0로 값이 나뉘지만 B1에는 영향을 끼치지 않는다.
이 한계는 더미 변수를 하나 추가함으로서 극복할 수 있다.
![](https://velog.velcdn.com/images/suwdle/post/793421ac-e6fc-4be2-b885-5bbbac0362c4/image.png)

student일 때 income의 기울기에 B3를 추가해 기울기를 갱신한다. 이는 소득에서의 변화가 신용카드 balance에 영향을 끼치는 것을 학생과 비학생에서 다르게 영향을 끼칠 수 있는 가능성을 허용한다.
![](https://velog.velcdn.com/images/suwdle/post/cb2c39b1-0d67-48e3-9d90-eae5ff23beae/image.png)
학생일 경우의 기울기가 더 작은데, 이 사실은 소득 증가가 비학생일 때보다 학생일 때 신용카드 balance와 더 작은 증가율로 연관되어 있다는 것을 알려준다.

### Non-linear Relationships
변수와 정답 사이의 관계가 비선형 관계인 경우가 있다. 이런 경우에 베이직한 선형 회귀 모델을 사용하게 될 경우 만족스러운 결과를 얻지 못할 것이다.
변수와 정답 사이의 관계가 비선형적인 것을 확인했을 때, polynomial 항을 추가해 비선형 함수를 사용할 수 있도록 한다. 하지만 여전히 선형적인 모델이다.
변수 X와 X^2에 대한 가중치를 예측하는 문제이기 때문이다. 쉽게 생각해, 단순히 X를 제곱한 값인 X^2를 X 축에 그리고, Y와의 관계를 나타내는 직선을 그려보자.
너무 많은 degree를 추가하는 것은 불필요하게 모델을 복잡하고 난해하게 만들 수 있고, 더 나아가 과대적합을 일으킬 수 있어 지양해야 한다.

### 3.3.3 Potential Problems
**Non-linearity of the Data**
선형회귀는 변수와 정답 사이의 직선형 관계를 가정하기 때문에 실제 관계가 그렇지 않다면 모델로부터의 예측값이 신뢰가 없고 정확도가 현저히 떨어질 것이다.
Residual plots은 비선형성을 밝혀낼 유용한 도구이다. 단순한 선형 회귀 모델에서 X에 대한 실제값과 예측값의 차이로 point를 찍어 plot을 만들고, 다중 모델일 때는 예측값과 잔차 사이의 plot을 만든다. 이상적으로는 residual plot은 알아볼 수 있는 패턴을 가지면 안 된다. 패턴의 존재는 선형 모델의 어떤 문제가 있다는 것을 나타낸다.
![](https://velog.velcdn.com/images/suwdle/post/4e2a870d-fd30-4976-bd79-a3d4983b8ea2/image.png)
Resudial plot이 비선형적인 관계를 나타낼 때 변수들에 대한 비선형적인 변환을 통해 변수를 추가하는 것이 가장 간단한 접근 방법이다.
**Correlation of Error Terms**

선형 회귀 모델의 중요한 가정 중 하나는 각 입실론 값들이 비상관관계에 있다는 것이다. 예를 들어, 이 값들이 비상관관계라면 각 입실론 값은 다른 입실론 값에 대해 정보를 제공할 수 없다. 표준 오차는 이 오차들이 비상관관계에 있다는 가정 하에 계산된다.
만약 상관관계가 있다면 표준오차 추정치는 실제 표준오차보다 과소평가되는 경향이 있다. 그 결과 신뢰 구간과 예측 구간이 좁아지게 될 것이다. 예를 들어 95%의 신뢰 구간이 실제로는 더 낮은 확률을 지니게 될 수 있다. 거기다 모델과 연관된 p-value는 실제보다 더 낮아질 것이다. 그 결과 우리는 모델에 대한 잘못된 확신을 가질 수 있다.

극단적인 예시로 데이터를 단순히 2배로 늘린 뒤 어떤 일이 일어나는지 보자. 표준오차 계산은 샘플 사이즈가 2n이기 때문에 더 작아질 것이다. 실제로는 n개의 샘플밖에 없는데도 말이다. 비슷하게 신뢰구간도 좁아질 것이다. [계산 참고](https://velog.io/@suwdle/ISL-Linear-Regression)
왜 오차 사이의 상관관계들이 발생할까? 이런 상관관계는 시계열 데이터에서 많이 발생하는데, 인접한 시간의 데이터 포인트들이 양의 상관관계를 가진 오차를 지니는 경우가 가장 많다. 
이 문제를 detection하기 위해 다시 resudial plot을 그려보자. x축은 시간의 흐름에 따른 observation이고, y축은 resudial이다. 
![](https://velog.velcdn.com/images/suwdle/post/76a02e2c-205c-40be-bac4-d052d91bcff7/image.png)
맨 위 차트처럼 상관관계가 없을 경우, 우리는 어떠한 경향성을 발견할 수 없다. 반대로 상관관계가 있는 경우 가까운 시간일수록 resudial의 값이 비슷한 경향성을 발견할 수 있다.

시계열 데이터가 아니더라도 이런 상관관계는 발견할 수 있다. 예를 들어, 몸무게를 통해 키를 예측하는 문제를 풀 경우를 생각해보자. 오차들이 비상관관계에 있다는 가정은 데이터 내에 같은 식단과 같은 환경 요인들을 공유하는 가족 구성원들이 있을 때 침범될 수 있다.

일반적으로, 실제로 상관관계가 존재할 수 있는 위험에도 불구하고 선형회귀를 비롯한 통계적 방법론에서 이런 가정은 무척 중요하기 때문에 이 위험을 해결하기 위한 좋은 모델 디자인을 필요로 한다.

**Non-constant Variance of Error Terms**
선형회귀 모델의 중요한 가정 중 또 다른 하나는 오차 항의 분산이 일정하다는 것이다. 등분산성이라고도 한다. 표준 오차, 신뢰 구간, 가설 검증에 사용되기 때문에 이 가정은 무척 중요하다. [표준오차](https://velog.io/@suwdle/ISL-Linear-Regression)

하지만 오차 항의 분산이 일정하지 않은 경우는 자주 있다. 예를 들어, 오차 항의 분산이 y값이 증가함에 따라 증가하는 경우가 있을 수 있다. 
![](https://velog.velcdn.com/images/suwdle/post/6ec97940-9885-430a-b679-d6d3c6196f0c/image.png)
왼쪽 차트가 그러한 문제의 발생을 보여준다. 이분산성(heteroscedasticity)이라고도 한다.
이런 문제를 직면했을 때 해결할 수 있는 방법 중 하나는 회귀 모델에 적용하기 전에 y값을 log나 루트를 씌워 전처리하는 것이다.
이런 변환은 더 큰 y값에 대해 더 큰 수축을 주기 때문에 자연스레 더 큰 값의 오차도 이런 효과를 받아 이분산성을 제어할 수 있다. 이런 변환이 효과적으로 적용된다면 데이터의 비선형적 관계가 있다는 하나의 증거로서도 볼 수 있다.

우리가 어떤 문제에 대한 모든 데이터를 가지고 있는 것이 아니기 때문에 오차 항의 분산도 한정된 데이터에서 계산한 추정치이다. 데이터 편향적인 추정으로 인해 이런 문제가 발생할 수도 있을 것 같다. 

> 이 내용은 **선형 회귀 모델**에서 **오차 항의 분산(heteroscedasticity)** 문제를 다룰 때 사용하는 기법을 설명하고 있습니다. 일반적인 선형 회귀 모델에서는 오차 항들이 동일한 분산을 가져야 한다고 가정합니다(등분산성). 하지만 실제 데이터에서는 각 응답(관측값)이 서로 다른 분산을 가질 때가 많습니다. 이때 그 분산의 차이를 반영하지 않으면 모델의 정확도가 떨어질 수 있습니다.
### 핵심 내용을 풀어서 설명하면:
1. **각 응답의 분산 추정**: 
   때로는 각 응답(또는 관측값)의 분산을 잘 추정할 수 있는 경우가 있습니다. 예를 들어, i번째 응답이 여러 개의 개별 관측값의 평균이라고 할 수 있습니다. 관측값이 여러 개 모이면, 평균의 분산은 개별 관측값의 분산보다 줄어듭니다. 
   - 만약 **개별 관측값**들의 분산이 2라고 가정하면, 그 관측값들의 평균은 더 작은 분산을 가집니다. i번째 응답이 ni개의 관측값들의 평균이라면, 평균의 분산은 **2/ni**가 됩니다.
2. **가중치 적용**: 
   이렇게 각 응답의 분산이 다를 때는 단순한 선형 회귀를 쓰기보다는, 각 응답에 가중치를 적용하는 방법을 사용할 수 있습니다. 이 방법이 **가중 최소 제곱법(Weighted Least Squares)**입니다. 가중치로는 **분산의 역수**를 사용하게 되는데, 이는 분산이 큰 응답보다 작은 응답이 모델에 더 큰 영향을 주도록 하기 위함입니다. 위의 예에서는 i번째 응답의 분산이 2/ni이므로, i번째 응답에 대한 가중치는 **ni**가 됩니다.
3. **회귀 소프트웨어에서의 적용**:
   대부분의 선형 회귀 소프트웨어는 이런 방식으로 각 관측값에 가중치를 적용할 수 있는 기능을 제공합니다. 가중치를 적용함으로써, 분산이 다른 데이터를 적절히 반영하여 더 정확한 모델을 만들 수 있습니다.
### 정리
결국, 데이터가 등분산성이 아닌 경우, 즉 응답값들의 분산이 서로 다를 때, **가중 최소 제곱법**을 사용하여 각 응답에 가중치를 부여하고 회귀를 수행하는 것이 좋습니다. 이는 모델의 예측 성능을 높이고, 편향된 결과를 줄여줍니다.


**Outliers**
이상치는 모델이 예측한 y값과 크게 차이가 나는 데이터 포인트를 말한다. 이상치는 여러가지 문제로 인해 생길 수 있는데, 이를테면 데이터 수집 중 관측값을 잘못 기록하는 경우가 있을 수 있다. 

데이터 수집이나 기록 과정에서 문제가 발생한 경우에는 이상치를 삭제하는 것이 가장 좋은 방법이다. 하지만 이상치를 바로 삭제하는 것은 주의를 요하는데, 이상치가 현재 데이터에 존재하지 않는 변수 등 모델의 부족함을 나타내는 것일 수도 있기 때문이다.

**High Leverage Point**
이상치가 일반적이지 않은 y값인 반면에 high leverage point는 일반적이지 않은 x값이다. high leverage point에 대한 resudial error가 높게 측정되어 추정 회귀직선을 만들 때 이런 포인트는 큰 영향을 끼치기 때문에 찾아내는 것이 중요하다.
![](https://velog.velcdn.com/images/suwdle/post/9b0b76c2-b06f-4a8b-b687-06092943ff89/image.png)
여기서 20은 이상치이고 41은 high leverage point이다. 맨 오른쪽 그래프에서 볼 수 있듯이, resudial과 leverage 모두 높게 나타난다.

단순 선형 회귀에서는 이런 값을 찾아내는 것이 쉽지만 다중 회귀에서는 어려울 수 있다. feature의 수가 2개보다 많을 경우 시각적으로 표현할 수 없기 때문이다.
각 포인트의 영향력을 수치화하기 위해 leverage-statics를 사용한다. 이 통계량의 수치가 클수록 포인트의 leverage가 커진다.
우선 단순 선형 회귀에서의 공식을 살펴보면:
![](https://velog.velcdn.com/images/suwdle/post/46c809a1-f813-42d0-8032-15cebad54a21/image.png)
이 공식에 따르면, xi와 x의 평균의 차이가 다른 x값에 비해 크다면(즉, xi가 일반적이지 않은 x값이라면) leverage-statics가 높게 계산된다. 이 값은 항상 1/n과 1사이로 계산되며 평균 영향력은 항상 (p+1)/n이다. 

**Colinearity**
공선성은 변수와 y사이의 선형관계가 아니라, 다변수 문제에서 두 개나 그 이상의 변수 사이의 다른 하나와 얼마나 연관되어 있는지에 대한 개념이다. 변수 사이의 상관관계가 강하게 존재할 때 우리는 그 변수들을 공선성을 가지고 있다고 말한다. 공선성의 존재는 해당 변수들의 y에 대한 영향력을 독립적으로 측정하는 것이 어렵기 때문에 회귀 모델에 치명적이다.
다시 말해 어떤 두 변수가 같이 증가하거나 감소하는 경향이 선형적으로 강하게 나타난다면 y와 어떻게 독립적으로 연관되는지 알기 어렵다.
![](https://velog.velcdn.com/images/suwdle/post/6bc44861-5e21-43a1-8865-97b0fb239b07/image.png)
오른쪽 그래프를 보면 비슷한 값의 RSS을 가진 지점들이 공선성 때문에 많다. 이런 경우, 데이터에 아주 작은 변화만 있어도 RSS를 최소화하는 계수를 찾지 못할 수 있다. 비슷한 값의 다른 지점으로 계산되기 쉽기 때문이다.
이런 영향 때문에 계수값이 제대로 계산되지 않아, 해당 feature들의 예측값에 대한 영향력이 매우 줄어들거나, y와의 상관관계를 전혀 반영하지 못할 수도 있다.
공선성이 회귀 계수 추정의 정확도를 낮추고, 계수에 대한 표준 오차는 증가시킨다. 그 결과 t-stastics가 증가해 공선성 때문에 귀무가설을 기각할 수 없게 된다. 간단히 말해, 공선성은 회귀 모델에 있어 치명적이다.

변수 사이의 공선성이 있는지 확인하는 방법을 알아보자. 우선 각각의 변수 사이의 상관관계를 담은 상관행렬을 통해 변수들이 1:1로 강한 상관관계, 즉 공선성을 가지고 있는지 확인할 수 있다. 

그러나 독립적으로 계산되는 상관관계는 공선성을 검사하기에 한계가 있는데, 상관계수가 높지 않더라도 세 개 이상의 변수 사이에서 공선성이 있을 수 있기 때문이다. 이를 **다중공선성**이라고 부르며 이를 검출하기 위해서 Variance Inflation Factor(VIF), 즉 분산 팽창 요인을 사용한다.
VIF는 전체 모델을 적합할 때의 분산을 추정 계수에 대해 적합할 때의 분산으로 나눈 값이다. VIF의 가장 작은 값은 1이고, 이 값은 완전한 공선성의 부재를 의미한다. VIF나 5나 10을 넘어가면 공선성 문제가 발생할 가능성이 높아진다.
![](https://velog.velcdn.com/images/suwdle/post/d4b4c73f-e95b-4e06-95da-d9b8a84ff17b/image.png)
여기서 R2는 다른 모든 변수에 대한 Xj의 회귀로 산출한 값이다. R2는 0부터 1까지의 값을 가지고, 1에 가까울수록 response에 대해 더 잘 나타내고 있는 것이므로, 이 R2가 커질수록 공선성 문제가 발생할 확률이 커질 것이다.

VIF를 통해 공선성 문제를 확인했다면, 어떻게 해결할 수 있을까? 일반적으로, 덜 중요하다고 여겨지는 변수를 삭제해서 공선성 문제를 해결한다. 대안적인 방법으로, 공선성 문제가 있는 두 변수를 병합해서 하나의 변수로 만들 수도 있다.