# Comparison with KNN
선형 회귀 모델은 선형 함수 f(X)를 추정하는 대표적인 parametric method로 데이터에 비해 적은 수의 계수를 지니고 있기 때문에 적합하기 쉽다. 또한 선형 회귀 모델의 경우 해석하기 쉬운 계수들을 가지고 있고 통계적 계산 또한 쉽게 구할 수 있다.

하지만 F(X)가 선형이라는 강력한 가정을 전제하고 이 가정이 틀리면 모델의 성능은 현저히 떨어질 것이다.

이런 단점이 없는 비모수적 방법론인 KNN regression을 선형 회귀 모델과 비교해 살펴보자.
KNN classifier와 무척 비슷한데, classifier가 K개의 이웃 중 이웃이 더 많이 포함된 label을 선택한다면, regression은 K개 이웃의 y값의 평균을 추정치로 사용한다.
![](https://velog.velcdn.com/images/suwdle/post/231adf06-03b2-493c-9e79-625b0b7d41ec/image.png)
일반적으로 K 값이 클수록 함수가 스무스해질 것이고 작게 설정할수록 계단 함수와 같은 형태가 될 것이다.
![](https://velog.velcdn.com/images/suwdle/post/71ccbc3c-d3be-4a20-a75b-771844e92832/image.png)
다른 말로 하면, K값이 클수록 데이터의 작은 변화에 민감하지 않고 less variable fit하게 된다. K값이 작을수록, flexible하지만 데이터의 변화에 민감하게 된다. K의 최적점을 찾는 것이 중요하다.

그럼 이제 어떤 상황에서 선형 회귀 모델 같은 모수적 방법론이 KNN 같은 비모수적 방법론보다 좋은 성능을 발휘할지 알아보자. 답은 간단하다. 선택된 모수적인 형태가 실제 f에 가깝다면 더 좋은 성능을 발휘한다.
그래프를 통한 예시를 간단히 살펴보자.
![](https://velog.velcdn.com/images/suwdle/post/d28e25d3-6ac0-4774-ad3c-01be42e3ae9c/image.png)

K=1일 때의 그래프는 지나친 다변성을 지니고 있고 K=9인 그래프는 그보다 안정적이고 스무스한 형태를 가지고 있다.
그러나 실제 관계가 선형적이라면 비모수적 방법론은 편향의 감소로 상쇄되지 않는 분산의 비용을 초래하기 때문에 선형회귀에 우위를 점하기 어렵다.
![](https://velog.velcdn.com/images/suwdle/post/ce781867-6361-4b76-96a2-9ed01f57b8f7/image.png)
우측 그래프에서 점선은 회귀모델의 MSE, K의 역수가 x축이기 때문에 증가할수록 k값이 작아지고 있다. k값이 클수록 mse가 작긴 하지만, 회귀모델을 능가하지는 못한다.

실전에서는 x와 y 사이의 관계가 정확히 선형일 때는 무척 드물다. 비선형적인 관계에서 둘 중 더 좋은 성능을 보이는 것을 찾아보자.
![](https://velog.velcdn.com/images/suwdle/post/5b3f98f5-07c5-4347-9ae1-f358cc5453ea/image.png)
위 그래프는 비선형적이지만 상대적으로 선형함수에 가까운 형태를 지니고 있어, 선형 회귀의 MSE가 전반적으로 낮게 나타나지만 일부 구간에서는 K값에 따라 KNN이 더 좋은 성능을 보여준다. 
비선형성이 더 강한 아래 그래프에서는 회귀 모델이 제대로 작동하지 않는다.
즉 polynomial feature를 따로 추가하지 않은 simple linear regression이라는 assumption 하에, 실제 데이터의 non-linearity가 강해질수록 KNN이 linear regression보다 좋은 성능을 보인다.

또 다른 관점에서 비교해보자. feature의 개수는 두 알고리즘의 성능에 어떤 영향을 끼칠까?
![](https://velog.velcdn.com/images/suwdle/post/e593d83f-8514-4ea5-8628-61c3adddc202/image.png)
p는 feature의 개수이다. 참고로 계속해서 추가되는 p들은 y와 관계가 없는 noise data이다. p가 1이거나 2일 때는 KNN이 더 나은 성능을 보여주지만 p가 커질수록 KNN의 성능이 매우 나빠진다. 하지만 linear regression은 계속 일정한 성능을 보여준다. coefficient만 늘어났을 뿐, 같은 작업을 반복하고 있기 때문이다. 

feature가 늘어난다는 건 데이터의 차원이 커진다는 의미이기도 하다. 차원이 커지면 데이터 포인트 사이의 거리가 멀어져 KNN이 가까운 이웃을 찾기 힘들어진다. 일종의 **차원의 저주**이다.

일반적으로 모수적 방법론이 비모수적 방법론보다 고차원 데이터를 처리할 때 유리하다.

만약 비모수적 방법론이 고차원 데이터를 처리하는 데 좋은 성능을 보이려면, 고차원을 밀도있게 채울만큼 많은 데이터를 필요로 할 것이다.

Linear regression은 설명가능한 계수를 가지고 있고, p-value 등의 통계 수치도 계산이 가능하기 때문에 데이터에 대한 분석과 통계의 관점에서도 KNN보다 유리하다.